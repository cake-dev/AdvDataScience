{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ch6 1, 8, (more to come)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "from statsmodels.api import OLS\n",
    "import sklearn.model_selection as skm\n",
    "import sklearn.linear_model as skl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ISLP import load_data\n",
    "from ISLP.models import ModelSpec as MS\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from ISLP.models import \\\n",
    "     (Stepwise,\n",
    "      sklearn_selected,\n",
    "      sklearn_selection_path)\n",
    "# !pip install l0bnb\n",
    "from l0bnb import fit_path # fit_path is a function that fits the l0bnb model, which is a model that fits a linear model with L0 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "We perform best subset, forward stepwise, and backward stepwise\n",
    "selection on a single data set. For each approach, we obtain p + 1\n",
    "models, containing 0, 1, 2, . . . , p predictors. Explain your answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a) Which of the three models with k predictors has the smallest\n",
    "training RSS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with k predictors that has the smallest training RSS will be the one that used best subset selection. This is because best subset selection considers all possible models with k predictors and selects the best one. Forward and backward stepwise selection do not consider all possible models with k predictors, so they may miss the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- b) Which of the three models with k predictors has the smallest test RSS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best subset selection would most likely have the smallest test RSS. This is because best subset selection considers all possible models with k predictors and selects the best one. Forward and backward stepwise selection do not consider all possible models, however they may still find the best model by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- c) True or False:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - i. The predictors in the k-variable model identified by forward\n",
    "stepwise are a subset of the predictors in the (k +1)-variable\n",
    "model identified by forward stepwise selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True.  The model with k+1 is an augmented version of the model with k predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - ii. The predictors in the k-variable model identified by back-\n",
    "ward stepwise are a subset of the predictors in the (k + 1)-\n",
    "variable model identified by backward stepwise selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True.  The model with k is created by removing a predictor from the model with k+1 predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - iii. The predictors in the k-variable model identified by back-\n",
    "ward stepwise are a subset of the predictors in the (k + 1)-\n",
    "variable model identified by forward stepwise selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False.  There is no connection here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - iv. The predictors in the k-variable model identified by forward\n",
    "stepwise are a subset of the predictors in the (k +1)-variable\n",
    "model identified by backward stepwise selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False. No connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - v. The predictors in the k-variable model identified by best\n",
    "subset are a subset of the predictors in the (k + 1)-variable\n",
    "model identified by best subset selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False.  The model with k+1 selects from all possible models with k+1 predictors, so it may not be a subset of the model with k predictors (predictors may be added or removed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8\n",
    "In this exercise, we will generate simulated data, and will then use\n",
    "this data to perform forward and backward stepwise selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a) Create a random number generator and use its normal() method\n",
    "to generate a predictor X of length n = 100, as well as a noise\n",
    "vector \" of length n = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numGen(n):\n",
    "    np.random.seed(42)\n",
    "    return np.random.normal(size=n)\n",
    "\n",
    "n = 100\n",
    "X = numGen(n)\n",
    "noise = numGen(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- b) Generate a response vector Y of length n = 100 according to the model\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\epsilon\n",
    "$$ ,\n",
    "where $\\beta_0$, $\\beta_1$, $\\beta_2$, and $\\beta_3$ are constants of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.array([4, 3, 2, 1])\n",
    "Y = beta[0] + beta[1]*X + beta[2]*X**2 + beta[3]*X**3 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "fig, ax = subplots()\n",
    "ax.scatter(X, Y)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_title('Scatter plot of X vs Y')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- c) Use forward stepwise selection to select a model containing the predictors $X, X^2, ... , X^{10}$. What is the model obtained according to $C_p$? Report the coeffecients of the model obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nCp(sigma2, estimator, X, Y): # negative Cp statistic\n",
    "    \"Negative Cp statistic\"\n",
    "    n, p = X.shape\n",
    "    Yhat = estimator.predict(X)\n",
    "    RSS = np.sum((Y - Yhat)**2)\n",
    "    return -(RSS + 2 * p * sigma2) / n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the predictors and the response\n",
    "df = pd.DataFrame({'Y': Y})\n",
    "for i in range(1, 11):\n",
    "    df[f'X^{i}'] = X**i\n",
    "# move the response to the last column\n",
    "df = df[['X^1', 'X^2', 'X^3', 'X^4', 'X^5', 'X^6', 'X^7', 'X^8', 'X^9', 'X^10', 'Y']]\n",
    "\n",
    "design = MS(df.columns.drop('Y')).fit(df) # fit a model spec to the data (MS is a class that represents a model specification, which is a set of features)\n",
    "Y = np.array(df['Y']) # the response variable\n",
    "X = design.transform(df) # the design matrix with predictors\n",
    "sigma2 = OLS(Y,X).fit().scale # the residual variance of the OLS model, .scale is the residual variance of the OLS model\n",
    "\n",
    "neg_Cp = partial(nCp, sigma2)\n",
    "\n",
    "strategy = Stepwise.first_peak(design,\n",
    "                               direction='forward',\n",
    "                               max_terms=len(design.terms))\n",
    "\n",
    "df_Cp = sklearn_selected(OLS,\n",
    "                               strategy,\n",
    "                               scoring=neg_Cp)\n",
    "df_Cp.fit(df, Y)\n",
    "df_Cp.selected_state_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefs from forward selection are the ones that were used to generate the data.  The added predictors are not significant, so they are not included in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- d) Repeat (c), using backwards stepwise selection. How does your\n",
    "answer compare to the results in (c)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m backward_strategy \u001b[38;5;241m=\u001b[39m Stepwise\u001b[38;5;241m.\u001b[39mfixed_steps(design, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     24\u001b[0m backward_selection \u001b[38;5;241m=\u001b[39m sklearn_selected(model, backward_strategy, scoring\u001b[38;5;241m=\u001b[39mneg_Cp)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mbackward_selection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m backward_selection\u001b[38;5;241m.\u001b[39mselected_state_\n",
      "File \u001b[0;32m~/miniconda3/envs/mathenv/lib/python3.10/site-packages/ISLP/models/sklearn_wrap.py:205\u001b[0m, in \u001b[0;36msklearn_selected.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_state_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselector_\u001b[38;5;241m.\u001b[39mselected_state_\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# now refit the model\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m Xsel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type(y, Xsel, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_args)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_\u001b[38;5;241m.\u001b[39mfit()\n",
      "File \u001b[0;32m~/miniconda3/envs/mathenv/lib/python3.10/site-packages/ISLP/models/generic_selector.py:317\u001b[0m, in \u001b[0;36mFeatureSelector.fit_transform\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit to training data then reduce X to its most important features.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, groups\u001b[38;5;241m=\u001b[39mgroups, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mathenv/lib/python3.10/site-packages/ISLP/models/generic_selector.py:284\u001b[0m, in \u001b[0;36mFeatureSelector.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fitted()\n\u001b[1;32m    283\u001b[0m build_submodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mbuild_submodel\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_submodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselected_state_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mathenv/lib/python3.10/site-packages/ISLP/models/model_spec.py:385\u001b[0m, in \u001b[0;36mModelSpec.build_submodel\u001b[0;34m(self, X, terms)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_submodel\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    366\u001b[0m                    X,\n\u001b[1;32m    367\u001b[0m                    terms):\n\u001b[1;32m    368\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    Build design on X after fitting.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m        Design matrix created with `terms`\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_info_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mterms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mintercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mencoders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoders_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mathenv/lib/python3.10/site-packages/ISLP/models/model_spec.py:624\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(column_info, X, terms, intercept, encoders)\u001b[0m\n\u001b[1;32m    621\u001b[0m         df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m    622\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m--> 624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m term_ \u001b[38;5;129;01min\u001b[39;00m terms:\n\u001b[1;32m    625\u001b[0m     term_df \u001b[38;5;241m=\u001b[39m build_columns(column_info,\n\u001b[1;32m    626\u001b[0m                             X,\n\u001b[1;32m    627\u001b[0m                             term_,\n\u001b[1;32m    628\u001b[0m                             col_cache\u001b[38;5;241m=\u001b[39mcol_cache,\n\u001b[1;32m    629\u001b[0m                             encoders\u001b[38;5;241m=\u001b[39mencoders,\n\u001b[1;32m    630\u001b[0m                             fit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    631\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(term_df)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# # create a dataframe with the predictors and the response\n",
    "# df = pd.DataFrame({'Y': Y})\n",
    "# for i in range(1, 11):\n",
    "#     df[f'X^{i}'] = X**i\n",
    "\n",
    "# # move the response to the last column\n",
    "# df = df[['X^1', 'X^2', 'X^3', 'X^4', 'X^5', 'X^6', 'X^7', 'X^8', 'X^9', 'X^10', 'Y']]\n",
    "\n",
    "# create the initial design matrix with all predictors\n",
    "design = MS(df.columns.drop('Y')).fit(df)\n",
    "Y = np.array(df['Y'])\n",
    "X = design.transform(df)\n",
    "\n",
    "# fit the full model and obtain the residual variance\n",
    "full_model = OLS(Y, X).fit()\n",
    "sigma2 = full_model.scale\n",
    "\n",
    "# define the scoring function\n",
    "neg_Cp = partial(nCp, sigma2)\n",
    "\n",
    "# perform backward subset selection\n",
    "model = OLS\n",
    "backward_strategy = Stepwise.fixed_steps(design, direction='backward', n_steps=10)\n",
    "backward_selection = sklearn_selected(model, backward_strategy, scoring=neg_Cp)\n",
    "backward_selection.fit(df, Y)\n",
    "backward_selection.selected_state_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- e) Now fit a lasso model to the simulated data, again using $X, X^2, ... , X^{10}$ as predictors. Use cross-validation to select the optimal\n",
    "value of λ. Create plots of the cross-validation error as a function\n",
    "of λ. Report the resulting coefficient estimates, and discuss the\n",
    "results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f) Now generate a response vector Y according to the model\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\epsilon\n",
    "$$\n",
    "and perform stepwise selection and the lasso. Discuss the results obtained."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mathenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
